{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f83d858-cd39-4e0b-b869-7042cfa540aa",
     "showTitle": true,
     "title": "all imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StringType,StructField,IntegerType\n",
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank,avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b62dfe-791e-4d34-807d-08e3e48c1587",
     "showTitle": true,
     "title": "read csv function"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv_data(path):\n",
    "    df=spark.read.csv(path,header=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a65039-8a86-4d7f-8e28-8be85a3581a1",
     "showTitle": true,
     "title": "write csv function"
    }
   },
   "outputs": [],
   "source": [
    "def write_csv_file(df,path):\n",
    "    df.write.format(\"csv\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebf9275-181d-4666-984d-7bf443187a43",
     "showTitle": true,
     "title": "read csv by custom schema"
    }
   },
   "outputs": [],
   "source": [
    "def read_schema(path,schema):\n",
    "    df=spark.read.csv(path=path,header=\"false\",schema=schema)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30eb5eed-5a0e-4b7f-bdd5-35bc0b93fa7e",
     "showTitle": true,
     "title": "read csv by option methord"
    }
   },
   "outputs": [],
   "source": [
    "def read_schema_options(path,schema):\n",
    "    df=spark.read.format(\"csv\").options(header=False).schema(schema).load(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8acd7cc-9f12-47f0-8aaa-19f076591adf",
     "showTitle": true,
     "title": "convert column name to snake case from Camel case"
    }
   },
   "outputs": [],
   "source": [
    "# 5. convert the Camel case of the columns to the snake case .\n",
    "\n",
    "def cample_to_snake(df):\n",
    "    for column in df.columns:\n",
    "        new=\"\"\n",
    "        for i in column:\n",
    "            if i.islower():\n",
    "                new=new+\"\".join(i)\n",
    "            else:\n",
    "                temp=f\"_{i}\"\n",
    "                new=new+\"\".join(temp.lower())\n",
    "        df = df.withColumnRenamed(column,new)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3e4d579-d0cb-4e44-97e8-9a0ccdc892f7",
     "showTitle": true,
     "title": "add column with current date"
    }
   },
   "outputs": [],
   "source": [
    "def add_load_data(df,col):\n",
    "    df=df.withColumn(col,current_date())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7323c29-9763-45a0-8f0d-25c2f82104fc",
     "showTitle": true,
     "title": "write df as delta table"
    }
   },
   "outputs": [],
   "source": [
    "def write_df(table_name,df,path):\n",
    "    df.write.option('path',path).saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1841534-1fe0-4cd1-b0f7-a1b9c410a09a",
     "showTitle": true,
     "title": "read delta table"
    }
   },
   "outputs": [],
   "source": [
    "def read_delta(path):\n",
    "    df = spark.read.format(\"delta\").load(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a66539-3983-4ff4-a5d8-916219231023",
     "showTitle": true,
     "title": "salary of each department in descending order."
    }
   },
   "outputs": [],
   "source": [
    "def find_sal_dep(delta_df):\n",
    "    windowPartition = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "    df=delta_df.withColumn(\"rank\",rank().over(windowPartition))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a9e78d-5bc9-45a7-9d3e-8daca3b1a514",
     "showTitle": true,
     "title": "department names along with their corresponding country names."
    }
   },
   "outputs": [],
   "source": [
    "def df_joins(employee_df,country_df,department_df):\n",
    "    df=employee_df.join(country_df,employee_df['country']==country_df['countryCode'],'inner')\n",
    "    df=df.join(department_df,df['department']==department_df['departmentId'],\"inner\")\n",
    "    return df.select(df.departmentName,df.countryName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bca0227-541d-48c1-8334-c08fd52de867",
     "showTitle": true,
     "title": "average age of employees in each department"
    }
   },
   "outputs": [],
   "source": [
    "def find_avg(delta_df,Window):\n",
    "    windowParti=Window.partitionBy('department')\n",
    "    df1=delta_df.withColumn(\"avg_age\",avg(delta_df['age']).over(windowParti))\n",
    "    return (df1.select(df1.department,df1.avg_age,df1.age)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c30560e-c151-4f03-a95e-2c925870dc04",
     "showTitle": true,
     "title": "write deltatable with replacewhere function"
    }
   },
   "outputs": [],
   "source": [
    "def write_df_gold(df_date,path):\n",
    "    df_date.write.mode(\"overwrite\").option(\"replaceWhere\", \"at_load_data >= '2024-01-01' \").save(path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3206574426058895,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
